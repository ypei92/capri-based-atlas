\section{Introduction}
\label{sec:intro}


Optimization parameters such as tile sizes and loop unrolling factors are critical to performance and it
is crucial to estimate the optimal value during optimization.
Currently programmers usually rely on optimization techiques provided by traditional compilers, which
is often too generalized to produce desired performance.
General compiler techniques cannot take care of applications such as general matrix multiply(GEMM),
whose performance is heavily based on those optimization parameters.
On the other hand, writing highly optimized code is impractical for average
programmers because it requires huge amount of prior knowledge both in application
itself and the target platform.


%Optimization parameters of high performance programs must be tuned for a given platform.
%how the program is written and how the optimization parameter is set.
\par

Auto-tuning is a straight-forward and one of the simplest way to find the optimal parameters.
There are two major categories of auto-tuning approaches: empirical search and model-based tuning.
\par
Empirical search tries to find the best performing code by searching every data point in
the parameter space and evaluate the performance on the actual hardware.
It has been successfully applied to build a variety
of high-performance domain-specific libraries including
dense linear algebra \cite{whaley2001automated}\cite{bilmes2014optimizing}, 
sparse linear algebra \cite{vuduc2005oski}, signal processing
\cite{frigo2005design}\cite{puschel2005spiral}, sorting \cite{li2004dynamically},
general stencil operations \cite{kamil2010auto}, etc.
\par
However, exaustively searching the whole parameter space is slow and impractical.
The search space is usually too large for modern machines and contains lots of low performance points where
searching is unnecessary. Consequently, most proposed searching methods
prune the space with hard-coded heuristics and may lose generality on new platforms.

\par
The second approach of auto-tuning is model-based tuning, which builds an analytical model
to estimates optimal parameters. The significant advantage of such approach is fast. Good optimization
parameters can be directly derived from the analytical model.
However in practice, it is generally difficult to build a highly accurate analytical model bacause of the growing complexity
of modern architectures and applications. The analytical model is inflexible and has to be built by expert of both architecture
and application.

\par
To address this weakness, it is intuitive to combine the two approaches together by first pruning the search
space with an automatically generated model, and then evaluate candidates
provided by the model to get the best set of parameters.
Additionally, the time to generate an optimized set of parameters
should also be taken into consideration. In some cases, users are not willing
to wait for a day or two, or even an hour to get the most optimized result. An
``good enough'' result is adequate. Therefore, approximation can be introduced
to accelerate the optimization process.

There is growing interest in approximate computing as a way of reducing energy
and time required to execute applications. \cite{ansel2011language,
baek2010green, sidiroglou2011managing, swaminathan2015case}. In conventional
computing, programs are usually treated as implementations of mathematical
functions, so there is a precise output for a given input.
However, in many problem domains, it is sufficient to produce an good ``enough''
result: for example, when rendering a image in graphics, it is
acceptable to take computational short-cuts if the graph doesn't hurt user's
experience. In addition, since analytical models are often imprecise for
applicatons with high complexity. Machine learning models can be a great
subsitute. In this paper, we use Capri\cite{sui2016proactive}, a published
approximate program, to build machine learning based models for \gem performance
based on different parameter settings. We denote parameter settings as knobs in
our paper. Capri would take past \atl running data and produce a machine
learning based model. \atl search process would take candidate knobs generated
by Capri and provide users with the best knob among the candidates knobs by
evaluating them exhaustively.


The contributions of this papers are:
\begin{itemize}
\item The performance of \gem can be well modeled by machine learning based
approaches provided by Capri.
\item The proposed Capri-based \atl can speed up the optimization process by XXX.
\item The performance of \gem using the knobs generated by Capri-based \atl
has average of 2\% performance degradation compared to the exhaustive search,
while the original \atl suffers 13\% performance degradation.
\end{itemize}

\par
The rest of this paper is organized as follows. Section \ref{sec:background} describes the background of ATLAS
orthogonal line search and Capri approximate ystem
Section \ref{sec:moti} adddresses our motivation for this project.
Section \ref{sec:design} introduces the implementation of Capri-based ATLAS search.
Section \ref{sec:experiment} provides the details of experiment methodology.
Section \ref{sec:evaluation} shows all the experiment results of our system, analyze and compare them with ATLAS orthogonal search.
Section \ref{sec:related} provides an overview of related work.
And finally, section \ref{sec:conclusion} concldes and outlines our possible future work.
